# 强化学习
## 1.主流强化学习算法
### 值函数逼近
略
### 策略搜索
随机策略:策略梯度，统计学习，路径积分  
AC算法:off-policy,使用actor网络(行动)和critic网络(评价).<a href="" target="_blank"><img src="img/1"/></a>  

DDPG算法 : 深度确定性策略，确定性体现在用确定性动作输出代替概率分布,使用QL评估。为打破数据相关性，经验回放和独立目标网络。
<a href="" target="_blank"><img src="img/2"/></a>

### 分布式强化学习算法
1.Gorila算法
这是基于异步的分布式的增强学习训练的框架，每个节点都有actor和自己完整的环境（包括replaymemory），还有learner用来采样、计算梯度（通常情况）。梯度是需要同步给集中的参数服务器的，参数服务器统一更新网络参数，然后周期性更新各个并行异步的环境。
<a href="" target="_blank"><img src="img/3"/></a>
缺点:replay buffer太耗计算，而且off policy更新模式有滞后性。

2.A3C算法
A3C的算法框架也是沿袭Gorila，但是不使用单独的节点来做agent和参数服务器，而是用了多核cpu来跑，在一台机器节点上跑，显然节省了大量的通讯开销（梯度和网络参数更新的量是非常巨大的）和无锁的方式更新参数和训练。而采用并行的不同的actors-learners就像在探索环境的不同部分，而且，每个人都可以采用不同的探索策略，这样的话能更全方位发现策略。通过不同的线程跑不同的探索策略，众多的actor-learners同时在线更新参数就像一个单独的agent更新一样，但是我们就不在需要replay memory和experience replay。因此采用这种算法有多个好处，首先减少了训练时间，我们可以发现训练的时间和线程数目基本上是线性关系。其次因为不再依赖experience relay可以让我们采用基于on-policy的增强学习算法。

3.PAAC算法
4.Ape-X
5.IMPALA

### 进化策略学习算法(ES算法)
1.梯度下降:对于监督学习，沿梯度快速。 
 进化算法:由原网络生成新网络，不断淘汰，可得全局最优。
 
 有研究表名，基于进化策略的强化学习能代替基于梯度的强化学习。
 
 


